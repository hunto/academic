
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5.0, minimum-scale=0.2">

<head>
    <link rel="stylesheet" href="bootstrap.min.css">
    <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101296995);</script>
    <script async src="//static.getclicky.com/js"></script>    

    <script>
        try{
            if (window.screen.width < 700) {
                setActiveStyleSheet("jemdoc_mobile.css"); 
            } 
            else if(/iPad/i.test(navigator.userAgent)){ 
                setActiveStyleSheet("jemdoc.css"); 
            } 
            else{
                setActiveStyleSheet("jemdoc.css"); 
            } 
        } 
        catch(e){} 

        function setActiveStyleSheet(filename){
            document.write("<link href="+filename+" rel=stylesheet>");
        }

        function checkFilter(type, li) {
            if (type == "All") {
                return true
            }
            else if (type == "First-authored") {
                res = li.getAttribute("first_authored")
                return res
            }
            else {
                cate = li.getAttribute("category")
                if (!cate) {
                    return false
                }
                items = cate.split(',')
                for (j = 0; j < items.length; j++) {
                    console.log(items[j])
                    if (type.toUpperCase() == items[j].toUpperCase()) {
                        return true
                    }
                }
                return false
            }
        }

        function filterPub(type) {
            ul = document.getElementById("publications")
            li = ul.getElementsByTagName("li")
            for (i = 0; i < li.length; i++) {
                if (!checkFilter(type, li[i])) {
                    li[i].style.display = "none";
                }
                else {
                    li[i].style.display = ""
                }
            }
            // change the button color
            bts = document.getElementsByClassName("filter")
            for (k = 0; k < bts.length; k++) {
                if (bts[k].textContent == type) {
                    bts[k].style.setProperty("--color", "#000")
                    bts[k].style.setProperty("--border", "#000")
                    // bts[k].style.color = "#000"
                }
                else {
                    bts[k].style.setProperty("--color", "#a0a0a0")
                    bts[k].style.setProperty("--border", "#d3d3d3")
                    // bts[k].style.color = "#a0a0a0"
                }
            }
        }

    </script>

    <script>
        // import data from './bibtex.json' assert { type: 'json' };
        const data = {
"huang2023masked": `@inproceedings{
huang2023masked,
title={Masked Distillation with Receptive Tokens},
author={Tao Huang and Yuan Zhang and Shan You and Fei Wang and Chen Qian and Jian Cao and Chang Xu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=mWRngkvIki3}
}`,
"huang2023knowledge": `@inproceedings{huang2023knowledge,
 author = {Huang, Tao and You, Shan and Wang, Fei and Qian, Chen and Xu, Chang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {33716--33727},
 publisher = {Curran Associates, Inc.},
 title = {Knowledge Distillation from A Stronger Teacher},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/da669dfd3c36c93905a17ddba01eef06-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}`,
"du2022quantum": `@article{du2022quantum,
  title={Quantum circuit architecture search for variational quantum algorithms},
  author={Du, Yuxuan and Huang, Tao and You, Shan and Hsieh, Min-Hsiu and Tao, Dacheng},
  journal={npj Quantum Information},
  volume={8},
  number={1},
  pages={62},
  year={2022},
  publisher={Nature Publishing Group UK London}
}`,
"huang2022greedynasv2": `@inproceedings{huang2022greedynasv2,
  title={Greedynasv2: greedier search with a greedy path filter},
  author={Huang, Tao and You, Shan and Wang, Fei and Qian, Chen and Zhang, Changshui and Wang, Xiaogang and Xu, Chang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11902--11911},
  year={2022}
}`,
"huang2022dyrep": `@inproceedings{huang2022dyrep,
  title={Dyrep: bootstrapping training with dynamic re-parameterization},
  author={Huang, Tao and You, Shan and Zhang, Bohan and Du, Yuxuan and Wang, Fei and Qian, Chen and Xu, Chang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={588--597},
  year={2022}
}`
        }

        const getBibTex = async (key) => {
            await navigator.clipboard.writeText(data[key]);
            alert("The following text was copied to your clipboard.\n===============\n"+data[key])
            // prompt("You can copy the text manually.", data[key]);
        }
    </script>

    <link rel="shortcut icon" href="myIcon.ico">
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="keywords" content="Tao Huang, SenseTime">
    <meta name="description" content="Tao Huang's home page">
    <!-- <link rel="stylesheet" href="jemdoc.css" type="text/css" />
    <link rel="stylesheet" media="screen and (max-width:700px)" href="jemdoc_mobile.css" type="text/css" />
    <link rel="stylesheet" media="screen and (max-width:700px)" href="jemdoc.css" type="text/css" /> -->
    <title>Tao Huang - Homepage</title>
</head>

<body>
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
    <nav class="navbar sticky-top navbar-expand-lg my-navbar-style"">
        <a class="navbar-brand" href="#" style="font-size: 120%; font-weight: bold; color: white;">Tao Huang</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
          <div class="navbar-nav">
            <a class="nav-item nav-link" href="#news">News</a>
            <a class="nav-item nav-link" href="#nav_publications">Publications</a>
            <a class="nav-item nav-link" href="#grants">Grants</a>
            <a class="nav-item nav-link" href="#talks">Talks</a>
            <a class="nav-item nav-link" href="#teaching">Teaching</a>
            <a class="nav-item nav-link" href="#services">Services</a>
            <!-- <a class="nav-item nav-link" href="#awards">Awards</a> -->
            <a class="nav-item nav-link" href="#education">Education</a>
          </div>
        </div>
    </nav>

    <script>
        // Wait for the DOM to load
        document.addEventListener("DOMContentLoaded", function () {
            // Get all navigation links
            const navLinks = document.querySelectorAll(".nav-link");
            const navbarToggler = document.querySelector(".navbar-toggler");
            const navbarCollapse = document.querySelector(".navbar-collapse");

            // Add click event listener to each navigation link
            navLinks.forEach(function (link) {
            link.addEventListener("click", function () {
                if (navbarCollapse.classList.contains("show")) {
                navbarToggler.click(); // Simulate a click on the toggler to close the menu
                }
            });
            });

            // Add click event listener to the document
            document.addEventListener("click", function (event) {
            // Check if the clicked element is not part of the menu or the toggler
            if (!navbarCollapse.contains(event.target) && !navbarToggler.contains(event.target)) {
                navbarCollapse.classList.remove("show"); // Hide the menu
            }
            });

            // Add mouseleave event listener to the menu
            navbarCollapse.addEventListener("mouseleave", function () {
            navbarCollapse.classList.remove("show"); // Hide the menu when the mouse leaves
            });
        });

    </script>

    <div id="layout-content" style="margin-top:0px">
        <table>
            <tbody>
                <tr>
                    <td width="60%" class="tdw" border="0">
                        <div id="toptitle">
                            <h1>Tao Huang</h1>
				        </div>
                        <p>
                            Ph.D. Student<br>
                            The University of Sydney
                        </p>
                        <p>
                            Email: <a href="mailto:huntocn@gmail.com">huntocn [at] gmail.com</a>
                        </p>
                        <p>
                            <a href="https://github.com/hunto"><img src="assets/github.png" class="icon"></a>&nbsp;&nbsp;
                            <a href="https://scholar.google.com/citations?user=jkcRdBgAAAAJ&hl=en"><img src="assets/google_scholar.svg" class="icon"></a>&nbsp;&nbsp;
                            <a href="https://www.linkedin.com/in/tao-huang-a93910292"><img src="assets/linkedin.svg" class="icon"></a>
                        </p>
                        <!-- <b>Control the controllable, observe the observable, leave the rest alone.</b> -->
                    </td>
                    <td class="photo"><img src="assets/taohuang.jpg" border="0" width="100%"></br></td>
			        
		<tr>
	</tbody>
</table>

<!-- <h2>Biography [<a href="assets/CV_taohuang.pdf">CV</a>]</h2> -->
<h2 style="margin-top: 0em;">Biography</h2>
<p>
    <div style="text-align:justify" class="paper"> 
        My name is Tao Huang (黄涛). I am a 3rd-year PhD student at <a href="https://www.sydney.edu.au/">The University of Sydney</a>, advised by <a href="http://changxu.xyz">Prof. Chang Xu</a>. 
        Before that, I was a Researcher at <a href="https://www.sensetime.com">SenseTime</a>, supervised by <a href="https://shanyou92.github.io/">Dr. Shan You</a>. 
        In 2020, I received my Bachelor's degree in Computer Science at <a href="http://english.hust.edu.cn/">Huazhong University of Science and Technology</a>, China.
    </div>
</p>
<p>My major research interests lie within <b>efficient deep learning</b> algorithms, such as
    <ul>
        <li style="margin-top: 0.2em">Efficient architectures: NAS, pruning, Rep, handcraft</li>
        <li style="margin-top: 0.2em">Knowledge distillation (KD)</li>
        <li style="margin-top: 0.2em">Efficient training</li>
        <li style="margin-top: 0.2em">Efficient large models</li>
    </ul>
</p>
<!-- <p>
    <b style="color: #D13644;">I am opening for job opportunities and collaborations, please drop me an email if interested.</b>
</p> -->

<!-- <p><font color="red">Pinned: </font></p> -->
<div id="news" style="margin-bottom: 2em; margin-top: -2em;"></div>
<h2>News</h2>
<ul>
    <li style="margin-top: 0.2em">[2024/03] Our <a href="">PU-MLC</a> was accepted to ICME 2024 as <b>oral</b>.</li>
    <li style="margin-top: 0.2em">[2024/03] We released <a href="https://arxiv.org/abs/2403.06517">ActGen</a> & <a href="https://arxiv.org/abs/2403.09338">LocalMamba</a> & <a href="https://arxiv.org/abs/2403.09977">EfficientVMamba</a> at arXiv.</li>
    <li style="margin-top: 0.2em">[2024/02] One paper (<a href="https://arxiv.org/abs/2311.12079">FreeKD</a>) was accepted to CVPR 2024.</li>
    <li style="margin-top: 0.2em">[2024/02] We released <a href="assets/dist+/DIST+.pdf">DIST+</a>.</li>
    <li style="margin-top: 0.2em">[2023/09] One paper (<a href="https://arxiv.org/abs/2305.15712">DiffKD</a>) was accepted to NeurIPS 2023.</li>
    <li style="margin-top: 0.2em">[2023/07] One paper (<a href="https://arxiv.org/abs/2305.02722">AKD</a>) was accepted to ACM MM 2023.</li>
    <li style="margin-top: 0.2em">[2023/05] We released <a href="https://arxiv.org/abs/2305.15712">DiffKD</a> and <a href="https://arxiv.org/abs/2305.02722">AKD</a> at arXiv.</li>
    <li style="margin-top: 0.2em">[2023/01] One paper (<a href="https://arxiv.org/abs/2205.14589">MasKD</a>) was accepted to ICLR 2023.</li>
    <li style="margin-top: 0.2em">[2022/09] One paper (<a href="https://arxiv.org/abs/2205.10536">DIST</a>) was accepted to NeurIPS 2022.</li>
</ul>
<button type="button" class="collapsible">More</button>
    <div class="content">
        <ul>
            <li style="margin-top: 0.2em">[2022/07] We released <a href="https://arxiv.org/abs/2207.05557">LightViT</a> <a href="https://github.com/hunto/LightViT">[code]</a> at arXiv.</li>
            <li style="margin-top: 0.2em">[2022/05] We released <a href="https://arxiv.org/abs/2205.10536">DIST</a> <a href="https://github.com/hunto/DIST_KD">[code]</a> and <a href="https://arxiv.org/abs/2205.14589">MasKD</a> <a href="https://github.com/hunto/MasKD">[code]</a> at arXiv.</li>
            <li style="margin-top: 0.2em">[2022/04] One paper (<a href="https://arxiv.org/abs/2010.10217">QAS</a>) was accepted to <i>Nature Partner Journals</i> Quantum Information (NPJ QI).</li>
            <li style="margin-top: 0.2em">[2022/03] Two papers (<a href="https://arxiv.org/abs/2111.12609">GreedyNASv2</a> & <a href="https://arxiv.org/abs/2203.12868">DyRep</a>) were accepted to CVPR 2022.</li>
            <li style="margin-top: 0.2em">[2022/01] One paper (<a href="https://arxiv.org/abs/2202.13197">ReLoss</a>) was accepted to ICLR 2022.</li>
            <li style="margin-top: 0.2em">
                [2021/12] We released <ax href="https://github.com/open-mmlab/mmrazor">MMRazor</ax> - a model compression toolkit for model slimming and AutoML, which includes 3 mainstream technologies NAS, pruning, and KD. MMRazor can be easily applied to various projects (e.g., MMDet and MMCls) in OpenMMLab.
            </li>
            <li style="margin-top: 0.2em">
                [2021/11] We released GreedyNASv2 at <ax href="https://arxiv.org/abs/2111.12609">arXiv</a>.
            </li>
            <li style="margin-top: 0.2em">
                [2021/03] One paper about NAS was accepted to CVPR 2021. The NAS benchmark in our paper was released at <ax href="https://github.com/xiusu/NAS-Bench-Macro">github</a>.
            </li>
            <li style="margin-top: 0.2em">
                [2021/01] One paper about channel number search (pruning) was accepted to ICLR 2021 as spotlight.
            </li>
            <li style="margin-top: 0.2em">
                [2020/11] One paper about NAS was released at <ax href="https://arxiv.org/abs/2011.09300">arXiv</a>. Our TopoNAS explicitly learns the topology for differentiable NAS (DARTS), and enjoys significant efficiency improvement on obtained architectures.
            </li>
            <li style="margin-top: 0.2em">
                [2020/10] One paper about quantum architecture search (QAS) was released at <ax href="https://arxiv.org/abs/2010.10217">arXiv</a>. Our QAS implicitly learns a rule that can well suppress the influence of quantum noise and the barren plateau.
            </li>
            <li style="margin-top: 0.2em">
                [2020/02] One paper about NAS was accepted to CVPR 2020.
            </li>
        </ul>
    </div>

<div id="nav_publications" style="margin-bottom: 2em; margin-top: -2em;"></div>
<h2>Publications</h2>
First-authored papers: <venue>CVPR</venue>x 4, <venue>NeurIPS</venue>x 2, <venue>ICLR</venue>x 2 &nbsp;&nbsp;&nbsp;&nbsp;*: co-first author. <br><br>

<button class="filter" type="button" onclick="filterPub('All')" style="--color: #000; --border: #000">All</button>&nbsp;
<button class="filter" type="button" onclick="filterPub('First-authored')">First-authored</button>&nbsp;
<button class="filter" type="button" onclick="filterPub('KD')">KD</button>&nbsp;
<button class="filter" type="button" onclick="filterPub('NAS')">NAS</button>&nbsp;
<button class="filter" type="button" onclick="filterPub('Pruning')">Pruning</button>
<ul id="publications">
    <li class="paper">
        <venue>KBS</venue><pt>Diverse and Tailored Image Generation for Zero-Shot Multi-Label Classification</pt><br>
        <div class="author">
            <g>Kaixin Zhang, Zhixiang Yuan, </g><u>Tao Huang</u><br>
            <em>Knowledge-Based Systems</em> (<b>KBS</b>), Volume 299, 5 September 2024, 112077.
        </div>
        <p>
            <a href="https://arxiv.org/abs/2404.03144" class="button-59">ArXiv</a>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705124007111" class="button-59">KBS</a>
            <a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=15960105280307206704" class="button-59">Bib</a>
            <a href="https://github.com/TAKELAMAG/Diff-ZS-MLC">
                <img alt="" src="https://github.com/TAKELAMAG/Diff-ZS-MLC?style=social" style="vertical-align: bottom; height: 1.2rem;"></a>
        </p>
    </li>
    <li class="paper">
        <venue>ICME</venue><pt>Positive Label Is All You Need for Multi-Label Classification</pt><flag>&#128681; Oral</flag><br>
        <div class="author">
            <g>Zhixiang Yuan, Kaixin Zhang, </g><u>Tao Huang</u><br>
            <em>IEEE International Conference on Multimedia and Expo</em> (<b>ICME</b>), 2024.
        </div>
        <p>
            <a href="https://arxiv.org/abs/2306.16016" class="button-59">ArXiv</a>
            <a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=11695637510257123983" class="button-59">Bib</a>
            <a href="https://github.com/TAKELAMAG/PU-MLC">
                <img alt="" src="https://img.shields.io/github/stars/TAKELAMAG/PU-MLC?style=social" style="vertical-align: bottom; height: 1.2rem;"></a>
        </p>
    </li>
    <li class="paper" category="KD">
        <venue>CVPR</venue><pt>FreeKD: Knowledge Distillation via Semantic Frequency Prompt</pt><br>
        <div class="author">
            <g>Yuan Zhang,</g> <u>Tao Huang</u><g>, Jiaming Liu, Tao Jiang, Kuan Cheng, Shanghang Zhang</g><br>
            <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2024.
        </div>
        <p>
            <a href="https://arxiv.org/abs/2311.12079" class="button-59">ArXiv</a>
            <a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=11695637510257123983" class="button-59">Bib</a>
        </p>
    </li>
    <li class="paper" first_authored=true category="KD">
        <venue>NeurIPS</venue><pt>Knowledge Diffusion for Distillation</pt><br>
        <div class="author">
            <u>Tao Huang</u><g>, Yuan Zhang, Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Chang Xu</g><br>
            <em>Advances in Neural Information Processing Systems</em> (<b>NeurIPS</b>), 2023.
        </div>
        <p>
            <a href="https://arxiv.org/abs/2305.15712" class="button-59">ArXiv</a>
            <a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=4615443208731882220" class="button-59">Bib</a>
            <a href="assets/diffkd/poster_neurips23_diffkd.pdf" class="button-59">Poster</a>
            <a href="https://github.com/hunto/DiffKD">
            <img alt="" src="https://img.shields.io/github/stars/hunto/DiffKD?style=social" style="vertical-align: bottom; height: 1.2rem;"></a>
        </p>
    </li>
    <li class="paper" category="KD">
        <venue>ACM MM</venue><pt>Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with Uncertainty</pt><br>
        <div class="author">
            <g>Yuan Zhang, Weihua Chen, Yichen Lu,</g> <u>Tao Huang</u><g>, Xiuyu Sun, Jian Cao</g><br>
            <em>Proceedings of the 31th ACM International Conference on Multimedia</em> (<b>ACM MM</b>), 2023.
        </div>
        <p>
            <a href="https://arxiv.org/abs/2305.02722" class="button-59">ArXiv</a>
            <a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=9322121606680381236" class="button-59">Bib</a>
            <a href="https://github.com/Gumpest/AvatarKD">
            <img alt="" src="https://img.shields.io/github/stars/Gumpest/AvatarKD?style=social" style="vertical-align: bottom; height: 1.2rem;"></a>
        </p>
    </li>
    <li class="paper" first_authored=true category="KD">
		<venue>ICLR</venue><pt>Masked Distillation with Receptive Tokens</pt><br>
        <div class="author">
            <u>Tao Huang</u>*<g>, Yuan Zhang*, Shan You, Fei Wang, Chen Qian, Jian Cao, Chang Xu</g><br>
            <em>International Conference on Learning Representations</em> (<b>ICLR</b>), 2023.</em>
        </div>
		<p>
			<a class="button-59" href="https://arxiv.org/abs/2205.14589">ArXiv</a>
            <button class="button-59" onclick="getBibTex('huang2023masked')">Bib</button>
            <a href="https://github.com/hunto/MasKD">
            <img alt="" src="https://img.shields.io/github/stars/hunto/MasKD?style=social" style="vertical-align: bottom; height: 1.2rem;"></a>
		</p>
    </li>
    <li class="paper" first_authored=true category="KD">
		<venue>NeurIPS</venue><pt>Knowledge Distillation from A Stronger Teacher</pt><br>
        <div class="author">
            <u>Tao Huang</u><g>, Shan You, Fei Wang, Chen Qian, Chang Xu</g><br>
            <em>Advances in Neural Information Processing Systems</em> (<b>NeurIPS</b>), 2022.
        </div>
		<p>
			<a href="https://arxiv.org/abs/2205.10536" class="button-59">ArXiv</a>
			<button class="button-59" onclick="getBibTex('huang2023knowledge')">Bib</button>
            <a href="assets/dist/KD_sharing_Tao_20220715.pdf" class="button-59">Slides</a>
            <a href="assets/dist/NeurIPS2022_DIST_poster.pdf" class="button-59">Poster</a>
            <a href="https://mp.weixin.qq.com/s/PwzyaZXCrl_W8NmiQgXm0g" class="button-59">解读</a>
            <a href="https://github.com/hunto/DIST_KD">
                <img alt="" src="https://img.shields.io/github/stars/hunto/DIST_KD?style=social" style="vertical-align: bottom; height: 1.2rem;">
            </a>
		</p>
    </li>
    <li class="paper" category="NAS">
		<venue>NPJ QI</venue><pt></pt><pt>Quantum circuit architecture search for variational quantum algorithms</pt><br>
        <div class="author">
            <g>Yuxuan Du,</g> <u>Tao Huang</u></b><g>, Shan You, Min-Hsiu Hsieh, Dacheng Tao</g><br>
		    <em>Nature Partner Journals Quantum Information</em> (<b>NPJ QI</b>), 2022.<br>
        </div>
		<p>
			<a href="https://arxiv.org/abs/2010.10217" class="button-59">ArXiv</a>
            <button class="button-59" onclick="getBibTex('du2022quantum')">Bib</button>
			<!-- <a href="https://scholar.google.com/scholar?cluster=14041544508923660633&hl=en&as_sdt=0,5" class="button-59">Bib</a> -->
			<a href="https://github.com/yuxuan-du/Quantum_architecture_search">
            <img alt="" src="https://img.shields.io/github/stars/yuxuan-du/Quantum_architecture_search?style=social" style="vertical-align: bottom; height: 1.2rem;">
            </a>
        </p>
    </li>
    <li class="paper" first_authored=true category="NAS">
		<venue>CVPR</venue><pt></pt><pt>GreedyNASv2: Greedier Search with a Greedy Path Filter</pt><br>
        <div class="author">
            <u>Tao Huang</u><g>, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang, Chang Xu</g><br>
            <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2022.<br>
        </div>
		<p>
			<a href="https://arxiv.org/abs/2111.12609" class="button-59">ArXiv</a>
			<!-- <a href="https://scholar.google.com/scholar?cluster=8646649449956447640&hl=en&as_sdt=0,5" class="button-59">Bib</a> -->
            <button class="button-59" onclick="getBibTex('huang2022greedynasv2')">Bib</button>
            <a href="assets/greedynasv2/CVPR2022_GreedyNASv2_Poster.pdf" class="button-59">Poster</a>
		</p>
    </li>
    <li class="paper" first_authored=true>
        <venue>CVPR</venue><pt>DyRep: Bootstrapping Training with Dynamic Re-parameterization</pt><br>
        <div class="author">
            <u>Tao Huang</u><g>, Shan You, Bohan Zhang, Yuxuan Du, Fei Wang, Chen Qian, Chang Xu</g><br>
            <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2022.<br>
        </div>
        <p>
            <a href="https://arxiv.org/abs/2203.12868" class="button-59">ArXiv</a>
            <!-- <a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=9004725926464672087" class="button-59">Bib</a> -->
            <button class="button-59" onclick="getBibTex('huang2022dyrep')">Bib</button>
            <a href="assets/dyrep/CVPR2022_DyRep_Poster.pdf" class="button-59">Poster</a>
            <a href="https://github.com/hunto/DyRep"><img alt="" src="https://img.shields.io/github/stars/hunto/DyRep?style=social" style="vertical-align: bottom; height: 1.2rem;"></a>
		</p>
    </li>
    <li class="paper" first_authored=true>
        <venue>ICLR</venue><pt>Relational Surrogate Loss Learning</pt><br>
        <div class="author">
            <u>Tao Huang</u><g>, Zekang Li, Hua Lu, Yong Shan, Shusheng Yang, Yang Feng, Fei Wang, Shan You, Chang Xu</g><br>
            <em>International Conference on Learning Representations</em> (<b>ICLR</b>), 2022.<br>
        </div>
        <p>
			<a href="https://arxiv.org/abs/2202.13197" class="button-59">ArXiv</a>
            <a href="https://openreview.net/forum?id=dZPgfwaTaXv" class="button-59">Bib</a>
            <a href="assets/reloss/Poster_ICLR2022_ReLoss.pdf" class="button-59">Poster</a>
            <a href="assets/reloss/Slides_ICLR2022_ReLoss.pdf" class="button-59">Slides</a>
            <a href="https://github.com/hunto/ReLoss"><img alt="" src="https://img.shields.io/github/stars/hunto/ReLoss?style=social" style="vertical-align: bottom; height: 1.2rem;"></a>
            
		</p>
    </li>
    <li class="paper" category="pruning">
		<venue>ICASSP</venue><pt>Data Agnostic Filter Gating for Efficient Deep Networks</pt><br>
        <div class="author">
            <g>Hongyan Xu, Xiu Su, Shan You,</g> <u>Tao Huang</u><g>, Fei Wang, Chen Qian, Changshui Zhang, Chang Xu, Dadong Wang, Arcot Sowmya</g><br>
            <em>IEEE International Conference on Acoustics, Speech and Signal Processing</em> (<b>ICASSP</b>), 2022.
        </div>
		<p>
			<a href="https://arxiv.org/abs/2010.15041" class="button-59">ArXiv</a>
			<a href="https://scholar.google.com/scholar?cluster=6802250960153223046&hl=en&as_sdt=0,5" class="button-59">Bib</a>
		</p>
	</li>
    <li class="paper" first_authored=true category="NAS">
        <venue>CVPR</venue><pt>Prioritized Architecture Sampling with Monto-Carlo Tree Search</pt><br>
        <div class="author">
            <g>Xiu Su*,</g> <u>Tao Huang</u>*<g>, Yanxi Li, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Chang Xu</g><br>
            <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2021.<br>
        </div>
        <p>
			<a href="https://arxiv.org/abs/2103.11922" class="button-59">ArXiv</a>
            <a href="assets/mctnas/Poster_CVPR2021_MCT-NAS.pdf" class="button-59">Poster</a>
            <a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=16897259555170724856" class="button-59">Bib</a>
            <a href="https://github.com/xiusu/NAS-Bench-Macro" class="button-59">NAS-Bench-Macro</a>
            <img alt="" src="https://img.shields.io/github/stars/xiusu/NAS-Bench-Macro?style=social" style="vertical-align: bottom; height: 1.2rem;">
		</p>
    </li>
    <li class="paper" category="NAS,pruning">
        <venue>ICLR</venue><pt>Locally Free Weight Sharing for Network Width Search</pt><flag>&#128681; Spotlight</flag><br>
        <div class="author">
            <g>Xiu Su, Shan You,</g> <u>Tao Huang</u><g>, Fei Wang, Chen Qian, Changshui Zhang, Chang Xu</g><br>
            <em>International Conference on Learning Representations</em> (<b>ICLR</b>), 2021.<br>
        </div>
        <p>
			<a href="https://arxiv.org/abs/2102.05258" class="button-59">ArXiv</a>
            <a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=13852905600525060682" class="button-59">Bib</a>
            <a href="https://openreview.net/forum?id=S0UdquAnr9k" class="button-59">OpenReview</a>
		</p>
    </li>
	<li class="paper" first_authored=true category="NAS">
		<venue>CVPR</venue><pt>GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet</pt><br>
        <div class="author">
            <g>Shan You*,</g> <u>Tao Huang</u>*<g>, Mingmin Yang*, Fei Wang, Chen Qian, Changshui Zhang</g><br>
            <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2020.</br>
        </div>
		<p>
			<a href="https://arxiv.org/abs/2003.11236" class="button-59">ArXiv</a>
            <a href="https://scholar.google.com/scholar?cluster=11467412928038806592&hl=en&as_sdt=0,5" class="button-59">Bib</a>
            <a href="assets/greedynas/Poster_CVPR2020_GreedyNAS.pdf" class="button-59">Poster</a>
            <a href="assets/greedynas/Video_CVPR2020_GreedyNAS.mp4" class="button-59">Video</a>
            <a href="assets/greedynas/Slides_GreedyNAS_titan.pdf" class="button-59">Slides</a>
            <a href="https://github.com/open-mmlab/mmrazor">
            <img alt="" src="https://img.shields.io/github/stars/open-mmlab/mmrazor?style=social" style="vertical-align: bottom; height: 1.2rem;"></a>
		</p>
	</li>
</ul>

<h2>Manuscripts</h2>
    <ul>
        <li class="paper">
            <venue>arXiv</venue><pt>Unveiling the Tapestry of Consistency in Large Vision-Language Models</pt><br>
            <div class="author">
                <g>Yuan Zhang, Fei Xiao, </g><u>Tao Huang</u><g>, Chun-Kai Fan, Hongyuan Dong, Jiawen Li, Jiacong Wang, Kuan Cheng, Shanghang Zhang, Haoyuan Guo</g><br>
                <em>arXiv preprint arXiv:2405.14156 (2024).</em>
            </div>
            <p>
                <a href="https://arxiv.org/abs/2405.14156" class="button-59">ArXiv</a>
                <a href="https://github.com/foundation-multimodal-models/ConBench"><img alt="" src="https://img.shields.io/github/stars/foundation-multimodal-models/ConBench?style=social" style="vertical-align: bottom; height: 1.2rem;"></a>
            </p>
        </li>
        <li class="paper">
            <venue>arXiv</venue><pt>EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba</pt><br>
            <div class="author">
                <g>Xiaohuan Pei*, </g><u>Tao Huang*</u><g>, Chang Xu</g><br>
                <em>arXiv preprint arXiv:2403.09977 (2024).</em>
            </div>
            <p>
                <a href="https://arxiv.org/abs/2403.09977" class="button-59">ArXiv</a>
                <a href="https://github.com/TerryPei/EfficientVMamba"><img alt="" src="https://img.shields.io/github/stars/TerryPei/EfficientVMamba?style=social" style="vertical-align: bottom; height: 1.2rem;"></a>
            </p>
        </li>
        <li class="paper">
            <venue>arXiv</venue><pt>LocalMamba: Visual State Space Model with Windowed Selective Scan</pt><br>
            <div class="author">
                <u>Tao Huang</u><g>, Xiaohuan Pei, Shan You, Fei Wang, Chen Qian, Chang Xu</g><br>
                <em>arXiv preprint arXiv:2403.09338 (2024).</em>
            </div>
            <p>
                <a href="https://arxiv.org/abs/2403.09338" class="button-59">ArXiv</a>
                <a href="https://github.com/hunto/LocalMamba"><img alt="" src="https://img.shields.io/github/stars/hunto/LocalMamba?style=social" style="vertical-align: bottom; height: 1.2rem;"></a>
            </p>
        </li>
        <li class="paper">
            <venue>arXiv</venue><pt>Active Generation for Image Classification</pt><br>
            <div class="author">
                <u>Tao Huang</u><g>, Jiaqi Liu, Shan You, Chang Xu</g><br>
                <em>arXiv preprint arXiv:2403.06517 (2024).</em>
            </div>
            <p>
                <a href="https://arxiv.org/abs/2403.06517" class="button-59">ArXiv</a>
            </p>
        </li>
        <li class="paper">
            <venue>arXiv</venue><pt>Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models</pt><br>
            <div class="author">
                <g>Wenhao Li, Xiu Su, Shan You,</g> <u>Tao Huang</u><g>, Fei Wang, Chen Qian, Chang Xu</g><br>
                <em>arXiv preprint arXiv:2312.13307 (2023).</em>
            </div>
            <p>
                <a href="https://arxiv.org/abs/2312.13307v2" class="button-59">ArXiv</a>
            </p>
        </li>
        <li class="paper" first_authored=true category="KD">
            <pt>DIST+: Knowledge Distillation from A Stronger Teacher</pt><br>
            <div class="author">
                <u>Tao Huang</u><g>, Shan You, Fei Wang, Chen Qian, Chang Xu</g><br>
            </div>
            <p>
                <a href="assets/dist+/DIST+.pdf" class="button-59">PDF</a>
            </p>
        </li>
        <li class="paper">
            <venue>arXiv</venue><pt>LightViT: Towards Light-Weight Convolution-Free Vision Transformers</pt><br>
            <div class="author">
                <u>Tao Huang</u><g>, Lang Huang, Shan You, Fei Wang, Chen Qian, Chang Xu</g><br>
                <em>arXiv preprint arXiv:2207.05557 (2022).</em>
            </div>
            <p>
                <a href="https://arxiv.org/abs/2207.05557" class="button-59">ArXiv</a>
                <a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=13876321258274905912" class="button-59">Bib</a>
                <a href="https://github.com/hunto/LightViT">
                <img alt="" src="https://img.shields.io/github/stars/hunto/LightViT?style=social" style="vertical-align: bottom; height: 1.2rem;"></a>
            </p>
        </li>
        <li class="paper">
            <venue>arXiv</venue><pt>Explicitly Learning Topology for Differentiable Neural Architecture Search</pt><br>
            <div class="author">
                <u>Tao Huang</u><g>, Shan You, Yibo Yang, Zhuozhuo Tu, Fei Wang, Chen Qian, Changshui Zhang</g><br>
                <em>arXiv preprint arXiv:2011.09300 (2020).</em>
            </div>
            <p>
                <a href="https://arxiv.org/abs/2011.09300" class="button-59">ArXiv</a>
            </p>
        </li>
    </ul>    

<div id="grants" style="margin-bottom: 2em; margin-top: -2em;"></div>
<h2>Grants</h2>
<ul>
    <li>2023-2024, Efficient Language-and-Vision Models, 1000 USD, Google Cloud Research Credits Award</li>
</ul>

<div id="talks" style="margin-bottom: 2em; margin-top: -2em;"></div>
<h2>Talks</h2>
<ul>
    <li>TNNLS 2024 Tutorial: "Efficient and Secure Foundation Models", Jun. 2024.</li>
    <li>TMLR Young Scientist Seminar@HKBU: "Knowledge Distillation from A Stronger Teacher", Jul. 2022. [<a href="assets/dist/KD_sharing_Tao_20220715.pdf">Slides</a>]</li>
    <li>SenseTime Titan Open Course: "Automated Machine Learning", May 2020. [<a href="https://b23.tv/MdQEF5P">Recording</a>]</li>
</ul>

<div id="teaching" style="margin-bottom: 2em; margin-top: -2em;"></div>
<h2>Teaching</h2>
<ul>
    <li>2024-2025, Guest Lecturer for COMP5329 Deep Learning, The University of Sydney</li>
    <li>2024-2025, Tutor for COMP5329 Deep Learning, The University of Sydney</li>
</ul>

<div id="services" style="margin-bottom: 2em; margin-top: -2em;"></div>
<h2>Academic Services</h2>
<b>Reviewer for Conferences:</b><br>
<div class="paper">NeurIPS 2021-2023, ICLR 2023-2024, ICML 2022-2024, CVPR 2022-2024, ICCV 2023, ECCV 2024, AAAI 2023-2024, ACM MM 2021-2023.</div><br>

<b>Reviewer for Journals:</b><br>
<div class="paper">TNNLS, TIP, IJCV</div>

<div id="education" style="margin-bottom: 2em; margin-top: -2em;"></div>
<h2>Education</h2>
<ul>
    <li>
        <div style="display: flex; justify-content: space-between;">
            <div><a href="https://www.sydney.edu.au/">The University of Sydney</a>, Sydney, Australia</div>
            <div style="margin-left: 2px;">Jul. 2022 – Jun. 2025 (expected)</div>
        </div>
        Ph.D. student at School of Computer Science<br>
        Advisor: <a href="http://changxu.xyz">Prof. Chang Xu</a><br>
    </li>
    <li>
        <div style="display: flex; justify-content: space-between;">
            <div><a href="https://www.hust.edu.cn">Huazhong University of Science and Technology</a>, Wuhan, China</div>
            <div style="margin-left: 2px;">Sep. 2016 – Jun. 2020</div>
        </div>
        B.E. in Computer Science and Technology<br>
    </li>
</ul>

<div id="experiences" style="margin-bottom: 2em; margin-top: -2em;"></div>
<h2>Selected Experiences</h2>
<ul>
	<li>
        <a href="https://sensetime.com">SenseTime</a>, Beijing, China<br>
        <div style="display: flex; justify-content: space-between; font-style: italic;">
            <div>Research Intern</div>
            <div style="margin-left: 2px;">Jul. 2022 - Present</div>
        </div>
        <div style="display: flex; justify-content: space-between; font-style: italic;">
            <div>Researcher</div>
            <div style="margin-left: 2px;">Jul. 2020 – Jun. 2022</div>
        </div>
        <div style="display: flex; justify-content: space-between; font-style: italic;">
            <div>Research Intern</div>
            <div style="margin-left: 2px;">Aug. 2019 – Jul. 2020</div>
        </div>
        <ul>
            <li style="margin-top: 0.2em">
                Research on model compression algorithms (NAS, KD, pruning, etc.).
            </li>
            <li style="margin-top: 0.2em">
                Research and development on auto-driving (smart carbin) scenes such as face verification and drowsiness detection.
            </li>
            <li style="margin-top: 0.2em">
                Applied NAS to face verification task on large­scale industrial datasets, which significantly improves the performance.
            </li>
        </ul>
	</li>
	<li>
        <a href="https://horizon.ai">Horizon Robotics</a>, Beijing, China
        <div style="display: flex; justify-content: space-between; font-style: italic;">
            <div>Computer Vision Research Intern</div>
            <div style="margin-left: 2px;">May 2019 – Aug. 2019</div>
        </div>
        <ul>
            <li style="margin-top: 0.2em">
                Development on object detection framework: anchor-­free detection method, detection in traffic scene.
            </li>
            <li style="margin-top: 0.2em">
                Research on knowledge distillation methods for face alignment, object detection.
            </li>
        </ul>
	</li>
	
</ul>
<button type="button" class="collapsible">More</button>
    <div class="content">
        <ul>
            <li>
                <a href="http://dian.org.cn/">Dian Group</a>, Wuhan, Hubei province, China<br>
                <div style="display: flex; justify-content: space-between; font-style: italic;">
                    <div>Team Leader of Real­time Face Detection & Alignment Project, AI Group</div>
                    <div style="margin-left: 2px;">Nov. 2018 – May 2019</div>
                </div>
                <ul>
                    <li style="margin-top: 0.2em">
                        Develop Android APP to inference face detection, tracking, and 106-­point landmark models on mobile devices. 
                    </li>
                    <li style="margin-top: 0.2em">
                        Research on model acceleration (e.g., knowledge distillation, model pruning) and facial landmark (e.g., multi­task learning, loss function, augmentation).
                    </li>
                    <li style="margin-top: 0.2em">
                        Our proposed model archieves an inference speed of 5 ms / image on Huawei Mate20 Pro.
                    </li>
                </ul>
                <div style="display: flex; justify-content: space-between; font-style: italic;">
                    <div>Core Member of Beibei Intelligent Customer Service Project, AI Group</div>
                    <div style="margin-left: 2px;">Feb. 2018 – Nov. 2018</div>
                </div>
                <ul>
                    <li style="margin-top: 0.2em">
                        This project comes from <a href="https://www.beibei.com/">Beibei Group Company</a>, Beibei is the bigest mother-­baby ecommerce platform in China. The task is to find optimal answers based on classifications of customer questions.
                    </li>
                    <li style="margin-top: 0.2em">
                        Research and development on text classification and data augmentation, etc.
                    </li>
                </ul>
            </li>
            <li>
                3D Printer Team, Wuhan, Hubei province, China
                <div style="display: flex; justify-content: space-between; font-style: italic;">
                    <div>Group Learder of Embedded Control Group</div>
                    <div style="margin-left: 2px;">Oct. 2016 – Jan. 2019</div>
                </div>
                Topic: developing control algorithms for 3DP/FDM 3D printers<br>
            </li>
        </ul>
    </div>

<div id="awards" style="margin-bottom: 2em; margin-top: -2em;"></div>
<h2>Selected Awards</h2>
<ul>
    <li>
        <div style="display: flex; justify-content: space-between;">
            <div>Outstanding Graduate award, Huazhong University of Science and Technology</div>
            <div style="margin-left: 2px;">Jun. 2020</div>
        </div>
    </li>
    <li>
        <div style="display: flex; justify-content: space-between;">
            <div>Outstanding Graduate Thesis award, Huazhong University of Science and Technology</div>
            <div style="margin-left: 2px;">Jun. 2020</div>
        </div>
    </li>
    <!-- <li>
        <div style="display: flex; justify-content: space-between;">
            <div>First Class Prize, National College Student Connected Smarter System Innovation Competition, National</div>
            <div style="margin-left: 2px;">2018</div>
        </div>
    </li>
    <li>
        <div style="display: flex; justify-content: space-between;">
            <div>Third Class Prize, "Challenge Cup" Competition, Provincial</div>
            <div style="margin-left: 2px;">2018</div>
        </div>
    </li>
    <li>
        <div style="display: flex; justify-content: space-between;">
            <div>First Class Prize, "Challenge Cup" Competition, HUST</div>
            <div style="margin-left: 2px;">2018</div>
        </div>
    </li> -->
</ul>

<div id="footer">
	<div id="footer-text" style="text-align: center;">© Tao Huang | Last updated: Feb 6, 2024</div>
</div>
	<!-- <center>© Tao Huang | Last updated: 02/19/2021</center> -->
    <!--<center>© Tao Huang
    	<script type="text/javascript" language="javascript">
    	if (Date.parse(document.lastModified) != 0) document.write(" | Last updated: " + document.lastModified);</script>
    </center>-->
</div>

</body>

<script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
        coll[i].addEventListener("click", function() {
            this.classList.toggle("active");
            var content = this.nextElementSibling;
            if (content.style.display === "block") {
            content.style.display = "none";
            } else {
            content.style.display = "block";
            }
        });
    }
</script>

</html>

